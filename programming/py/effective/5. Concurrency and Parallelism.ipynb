{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concurrency and Parallelism\n",
    "\n",
    "**Concurrency is when a computer does many different things `seemingly` at the same time**. For example, on a computer with one CPU core, the operating system will rapidly change which program is running on the single processor. This interleaves execution of the programs, providing the illusion that the programs are running simultaneously.\n",
    "\n",
    "**Parallelism is actually doing many different things at the same time**. Computers with multiple CPU cores can execute multiple programs simultaneously. Each CPU core runs the instructions of a separate program, allowing each program to make forward progress during the same instant.\n",
    "\n",
    "Within a single program, concurrency is a tool that makes it easier for programmers to solve certain types of problems. **Concurrent programs enable many distinct paths of execution to make forward progress** in a way that seems to be both simultaneous and independent.\n",
    "\n",
    "The key difference between parallelism and concurrency is **speedup**. When two distinct paths of execution in a program make forward progress in parallel, the time it takes to do the total work is cut in half; the speed of execution is faster by a factor of two. In contrast, concurrent programs may run thousands of separate paths of execution seemingly in parallel but provide no speedup for the total work.\n",
    "\n",
    "**Python makes it easy to write concurrent programs**. Python can also be used to do parallel work through system calls, subprocesses, and C-extensions. But **it can be very difficult to make concurrent Python code truly in parallel**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 36: Use subprocess to Manage Child Processes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 37: Use Threads for Blocking I/O, Avoid for Parallelism\n",
    "\n",
    "The standard implementation of Python is called *CPython*. **CPython runs a Python program in two steps**. First, it parses and compiles the source text into bytecode. Then, **it runs the bytecode using a stack-based interpreter**. The bytecode interpreter has state that must be maintained and coherent while the Python program executes. Python enforces coherence with a mechanism called the **global interpreter lock (GIL)**.\n",
    "\n",
    "Essentially, the GIL is a mutual-exclusion lock (mutex) that prevents CPython from being affected by preemptive multithreading, where one thread takes control of a program by interrupting another thread.\n",
    "\n",
    "The GIL has an important negative side effect. With programs written in languages like C++ or Java, having multiple threads of execution means your program could utilize multiple CPU cores at the same time. Although Python supports multiple threads of execution, **the GIL causes only one of them to make forward progress at a time**. This means that when you reach for threads to do parallel computation and speed up your Python programs, you will be sorely disappointed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 1.185 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "# computationally intensive\n",
    "def factorize(number):\n",
    "    for i in range(1, number+1):\n",
    "        if number % i == 0:\n",
    "            yield i\n",
    "            \n",
    "numbers = [2139079, 1214759, 1516637, 1852285, 2139079]\n",
    "start = time()\n",
    "for n in numbers:\n",
    "    list(factorize(n))\n",
    "end = time()\n",
    "print('Took %.3f seconds' % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 1.235 seconds\n"
     ]
    }
   ],
   "source": [
    "# do the same comp as before\n",
    "from threading import Thread\n",
    "\n",
    "class FactorizeThread(Thread):\n",
    "    def __init__(self, number):\n",
    "        super().__init__()\n",
    "        self.number = number\n",
    "        \n",
    "    def run(self):\n",
    "        self.factors = list(factorize(self.number))\n",
    "        \n",
    "numbers = [2139079, 1214759, 1516637, 1852285, 2139079]\n",
    "start = time()\n",
    "\n",
    "threads = []\n",
    "for n in numbers:\n",
    "    thread = FactorizeThread(n)\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "    \n",
    "# wait for all threads\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "\n",
    "end = time()\n",
    "print('Took %.3f seconds' % (end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's surprising is that this takes even longer than running `factorize` in serial. You may wonder, why does Python support threads at all? There are two good reasons.\n",
    "\n",
    "First, multiple threads make it easy for your program to seem like it’s doing multiple things at the same time. Managing the juggling act of simultaneous tasks is difficult to implement yourself (see Item 40: “Consider Coroutines to Run Many Functions Concurrently” for an example). **With threads, you can leave it to Python to run your functions seemingly in parallel**.\n",
    "\n",
    "The second reason Python supports threads is to **deal with blocking I/O**, which happens when Python does certain types of system calls. System calls are how your Python program asks your computer’s operating system to interact with the external environment on your behalf. Blocking I/O includes things like reading and writing files, interacting with networks, communicating with devices like displays, etc. Threads help you handle blocking I/O by insulating your program from the time it takes for the operating system to respond to your requests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 0.531 seconds\n"
     ]
    }
   ],
   "source": [
    "import select\n",
    "\n",
    "def slow_syscall():\n",
    "    select.select([], [], [], 0.1)\n",
    "    \n",
    "start = time()\n",
    "for _ in range(5):\n",
    "    slow_syscall()\n",
    "    \n",
    "end = time()\n",
    "print('Took %.3f seconds' % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "Took 0.107 seconds\n"
     ]
    }
   ],
   "source": [
    "# run multiple invocations in separate threads\n",
    "start = time()\n",
    "threads = []\n",
    "for _ in range(5):\n",
    "    thread = Thread(target = slow_syscall)\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "    \n",
    "def compute_loc(i):\n",
    "    print(i)\n",
    "    \n",
    "for i in range(5):\n",
    "    compute_loc(i)\n",
    "    \n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "    \n",
    "end = time()\n",
    "print('Took %.3f seconds' % (end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parallel time is 5× less than the serial time. This shows that the system calls will all run in parallel from multiple Python threads even though they’re limited by the GIL. The GIL prevents my Python code from running in parallel, but it has no negative effect on system calls. This works because **Python threads release the GIL just before they make system calls and reacquire the GIL as soon as the system calls are done**.\n",
    "\n",
    "There are many other ways to deal with blocking I/O besides threads, such as the **asyncio built-in module**, and these alternatives have important benefits. But these options also require extra work in refactoring your code to fit a different model of execution (see Item 40: “Consider Coroutines to Run Many Functions Concurrently”). **Using threads is the simplest way to do blocking I/O in parallel with minimal changes to your program**.\n",
    "\n",
    "Things to Remember\n",
    "* Python threads can’t run bytecode in parallel on multiple CPU cores because of the global interpreter lock (GIL).\n",
    "* Python threads are still useful despite the GIL because they provide an easy way to do multiple things at seemingly the same time.\n",
    "* Use Python threads to make multiple system calls in parallel. This allows you to do blocking I/O at the same time as computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 38: Use Lock to Prevent Data Races in Threads\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 39: Use Queue to Coordinate Work Between Threads\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
